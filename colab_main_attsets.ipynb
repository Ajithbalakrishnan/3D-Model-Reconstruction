{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_main_attsets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajithbalakrishnan/3D-Model-Reconstruction/blob/wipropc/colab_main_attsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goO7LQkROpCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "226ab1c8-ddfe-4186-d268-64c0db984a99"
      },
      "source": [
        "!pip install keras tensorflow scipy pandas numpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (3.0.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmqG3U40PR6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9burQR2bZijh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77ec78ec-e968-4380-8fb8-ebd40b270b72"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import scipy.io\n",
        "sys.path.append('..')\n",
        "import tools as tools\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.layers import BatchNormalization,Conv3D,MaxPooling3D,Dense,Reshape,Add,LeakyReLU\n",
        "from keras.activations import relu,sigmoid\n",
        "from keras import models\n",
        "import copy "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpB1i-x-ZyfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1\n",
        "img_res = 127\n",
        "vox_res32 = 32\n",
        "total_mv = 24   \n",
        "GPU0 = '0'\n",
        "#re_train=False\n",
        "re_train=True\n",
        "single_view_train = False\n",
        "multi_view_train = False\n",
        "\n",
        "#####################################\n",
        "\n",
        "config={}                                 # python dictionary\n",
        "config['batch_size'] = batch_size\n",
        "config['total_mv'] = total_mv\n",
        "#config['cat_names'] = ['02691156','02828884','02933112','02958343','03001627','03211117',\n",
        "#            '03636649','03691459','04090263','04256520','04379243','04401088','04530566']\n",
        "config['cat_names'] = ['03001627']\n",
        "for name in config['cat_names']:\n",
        "    config['X_rgb_'+name] = './Data_sample/ShapeNetRendering/'+name+'/'\n",
        "    config['Y_vox_'+name] = './Data_sample/ShapeNetVox32/'+name+'/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKxLR8IjaA-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_IoU(batch_voxel_occup_pred, batch_voxel_occup_true):\n",
        "    batch_voxel_occup_pred_ = copy.deepcopy(batch_voxel_occup_pred)\n",
        "    batch_voxel_occup_pred_[batch_voxel_occup_pred_ >= 0.5] = 1\n",
        "    batch_voxel_occup_pred_[batch_voxel_occup_pred_ < 0.5] = 0\n",
        "\t\n",
        "    I = batch_voxel_occup_pred_ * batch_voxel_occup_true\n",
        "    U = batch_voxel_occup_pred_ + batch_voxel_occup_true\t\t\t\n",
        "    U[U < 1] = 0\n",
        "    U[U >= 1] = 1\n",
        "    iou = np.sum(I) * 1.0 / np.sum(U) * 1.0\n",
        "    return iou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l9pnPOzaCQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def refiner_network(volumes_in):\n",
        "\t\n",
        "\tinput_volumes_32 = tf.reshape(volumes_in, [-1, vox_res32, vox_res32, vox_res32, 1])\n",
        "\t\n",
        "\tprint(\"input_volumes_32_shape\" , input_volumes_32.shape)   #input_volumes_32_shape (?,32,32,32,1)\n",
        "\t\n",
        "\trn1=Conv3D(filters=32, kernel_size=(4, 4, 4), padding='same',data_format=\"channels_last\",name='ref_c1')(input_volumes_32)\n",
        "\trn2=BatchNormalization()(rn1)\n",
        "\trn3=LeakyReLU(alpha=.2)(rn2)\n",
        "\tvolumes_16_l =MaxPooling3D(pool_size=(2, 2, 2),name='ref_m1')(rn3)\n",
        "\t\n",
        "\tprint(\"volumes_16_l_shape\" , volumes_16_l.shape)      #volumes_16_l_shape (?,16,16,16,32)\n",
        "\t\n",
        "\trn5=Conv3D(filters=64, kernel_size=(4, 4, 4), padding='same',data_format=\"channels_last\",name='ref_c2')(volumes_16_l)\n",
        "\trn6=BatchNormalization()(rn5)\n",
        "\trn7=LeakyReLU(alpha=.2)(rn6)\n",
        "\tvolumes_8_l =MaxPooling3D(pool_size=(2, 2, 2),name='ref_m2')(rn7)\n",
        "\t\n",
        "\tprint(\"volumes_8_l_shape\" ,volumes_8_l.shape)\n",
        "\t\n",
        "\trn9=Conv3D(filters=128, kernel_size=(4, 4, 4), padding='same',data_format=\"channels_last\",name='ref_c3')(volumes_8_l)\n",
        "\trn10=BatchNormalization()(rn9)\n",
        "\trn11=LeakyReLU(alpha=.2)(rn10)\n",
        "\tvolumes_4_l =MaxPooling3D(pool_size=(2, 2, 2),name='ref_m3')(rn11)\n",
        "\t\n",
        "\tprint(\"volumes_4_l_shape\" , volumes_4_l.shape)\n",
        "\t\n",
        "\tflatten_features=tf.reshape(volumes_4_l , [-1,8192])   \n",
        "\t\n",
        "\tfc1=Dense(units=2048, activation='relu',name='ref_fc1')(flatten_features)\n",
        "\tfc1=relu(fc1, alpha=0.0, max_value=None, threshold=0.0)\n",
        "\t\n",
        "\tprint(\"fc1_shape\",fc1.shape)\n",
        "\t\n",
        "\tfc2=Dense(units=8192, activation='relu',name='ref_fc2')(fc1)\n",
        "\tfc2=relu(fc2, alpha=0.0, max_value=None, threshold=0.0)\n",
        "\t\n",
        "\tprint(\"fc2_shape\",fc2.shape)\n",
        "\t\n",
        "\tfc2=tf.reshape(fc2, [-1, 4,4,4,128])        \n",
        "\treshaped_1=Add()([fc2,volumes_4_l]) \n",
        "\t\n",
        "\tprint(\"reshaped_1.shape\",reshaped_1.shape)\n",
        "\t\n",
        "\trn13= tools.Ops.deconv3d(reshaped_1, k=4, out_c=64, str=2, name='ref_c4')\n",
        "\trn14=BatchNormalization()(rn13)\n",
        "\tvolumes_4_r=relu(rn14, alpha=0.0, max_value=None, threshold=0.0)\n",
        "\t\n",
        "\tprint(\"volumes_4_r_shape\",volumes_4_r.shape)\n",
        "\t\n",
        "\treshaped_2=Add() ([volumes_4_r,volumes_8_l]) \n",
        "\t\n",
        "\tprint(\"reshaped_2_shape\",reshaped_2.shape)\n",
        "\n",
        "\trn16= tools.Ops.deconv3d(reshaped_2, k=4, out_c=32, str=2, name='ref_c5')\n",
        "\trn17=BatchNormalization()(rn16)\n",
        "\tvolumes_8_r =relu(rn17, alpha=0.0, max_value=None, threshold=0.0)\n",
        " \n",
        "\treshaped_3=Add()([volumes_8_r,volumes_16_l])\n",
        "    \n",
        "\tprint(\"reshaped_3_shape\",reshaped_3.shape)\n",
        "\t\n",
        "\trn19= tools.Ops.deconv3d(volumes_8_r, k=4, out_c=1, str=2, name='ref_c6')\n",
        "\tvolumes_16_r= sigmoid(rn19)\n",
        "\n",
        "\treshape_4=Add()([volumes_16_r,input_volumes_32])\n",
        "\treshape_4=(reshape_4*0.5)\n",
        "\t\n",
        "\tprint(\"reshape_4_shape\",reshape_4.shape)\n",
        "\t\n",
        "\treshape_5=tf.reshape(reshape_4, [-1, vox_res32, vox_res32, vox_res32])\n",
        "\n",
        "\treturn reshape_5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2FM5L7RaHej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attsets_fc(x, out_ele_num,  name):\n",
        "    in_ele_num = tf.shape(x)[1]\n",
        "    in_ele_len = int(x.get_shape()[2])\n",
        "    out_ele_len = in_ele_len    \n",
        "    print(\"out_ele_len \", out_ele_len)\n",
        "\n",
        "    x_1st = x\n",
        "    x_1st_tp = tf.reshape(x_1st, [-1, in_ele_len])\n",
        "    weights_1st = tools.Ops.fc(x_1st_tp, out_d=out_ele_num*out_ele_len, name=name+'_1st')\n",
        "\t\n",
        "\t########## option 1\n",
        "    weights_1st = weights_1st\n",
        "\t########## option 2\n",
        "\t#weights_1st = tf.nn.tanh(weights_1st)\n",
        "\n",
        "\n",
        "    weights_1st = tf.reshape(weights_1st, [-1, in_ele_num, out_ele_num, out_ele_len])\n",
        "    weights_1st = tf.nn.softmax(weights_1st, 1)\n",
        "    x_1st = tf.tile(x_1st[:,:,None,:], [1,1,out_ele_num,1])\n",
        "    x_1st = x_1st*weights_1st\n",
        "    x_1st = tf.reduce_sum(x_1st, axis=1)\n",
        "    x_1st = tf.reshape(x_1st, [-1, out_ele_num*out_ele_len])       \n",
        "    return x_1st, weights_1st"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93M7HtXZaNy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "\tdef __init__(self):\n",
        "\t\tself.train_mod_dir = './train_mod/'\n",
        "\t\tself.train_sum_dir = './train_sum/'\n",
        "\t\tself.test_res_dir = './test_res/'\n",
        "\t\tself.test_sum_dir = './test_sum/'\n",
        "\t\t\n",
        "\t\tprint ('re_train : ', re_train)\n",
        "\t\tif os.path.exists(self.test_res_dir):\n",
        "\t\t\tif re_train:\n",
        "\t\t\t\tprint ('test_res_dir and files kept!')\n",
        "\t\t\telse:\n",
        "\t\t\t\tshutil.rmtree(self.test_res_dir)\n",
        "\t\t\t\tos.makedirs(self.test_res_dir)\n",
        "\t\t\t\tprint ('test_res_dir: deleted and then created!')\n",
        "\t\telse:\n",
        "\t\t\tos.makedirs(self.test_res_dir)\n",
        "\t\t\tprint ('test_res_dir: created!')\n",
        "\t\t\n",
        "\t\tif os.path.exists(self.train_mod_dir):\n",
        "\t\t\tif re_train:\n",
        "\t\t\t\tif os.path.exists(self.train_mod_dir + 'model.cptk.data-00000-of-00001'):\n",
        "\t\t\t\t\tprint ('model found! will be reused!')\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tprint ('model not found! error!')\n",
        "\t\t\t\t\t#exit()\n",
        "\t\t\telse:\n",
        "\t\t\t\tshutil.rmtree(self.train_mod_dir)\n",
        "\t\t\t\tos.makedirs(self.train_mod_dir)\n",
        "\t\t\t\tprint ('train_mod_dir: deleted and then created!')\n",
        "\t\telse:\n",
        "\t\t\tos.makedirs(self.train_mod_dir)\n",
        "\t\t\tprint ('train_mod_dir: created!')\n",
        "\t\t\n",
        "\t\tif os.path.exists(self.train_sum_dir):\n",
        "\t\t\tif re_train:\n",
        "\t\t\t\tprint ('train_sum_dir and files kept!')\n",
        "\t\t\telse:\n",
        "\t\t\t\tshutil.rmtree(self.train_sum_dir)\n",
        "\t\t\t\tos.makedirs(self.train_sum_dir)\n",
        "\t\t\t\tprint ('train_sum_dir: deleted and then created!')\n",
        "\t\telse:\n",
        "\t\t\tos.makedirs(self.train_sum_dir)\n",
        "\t\t\tprint ('train_sum_dir: created!')\n",
        "\t\t\n",
        "\t\tif os.path.exists(self.test_sum_dir):\n",
        "\t\t\tif re_train:\n",
        "\t\t\t\tprint ('test_sum_dir and files kept!')\n",
        "\t\t\telse:\n",
        "\t\t\t\tshutil.rmtree(self.test_sum_dir)\n",
        "\t\t\t\tos.makedirs(self.test_sum_dir)\n",
        "\t\t\t\tprint ('test_sum_dir: deleted and then created!')\n",
        "\t\telse:\n",
        "\t\t\tos.makedirs(self.test_sum_dir)\n",
        "\t\t\tprint ('test_sum_dir: created!')\n",
        "\n",
        "\tdef base_r2n2(self, X_rgb):\n",
        "\t\tim_num = tf.shape(X_rgb)[1]\n",
        "\t\t[_, _, d1, d2, cc] = X_rgb.get_shape()\n",
        "\t\tX_rgb = tf.reshape(X_rgb, [-1, int(d1), int(d2), int(cc)])\n",
        "\t\tprint(\"Network Structure\")\n",
        "\t\tprint(\"base_r2n2\",X_rgb.shape)\n",
        " \n",
        "\t\ten_c = [96, 128, 256, 256, 256, 256]\n",
        "\t\tl1 = tools.Ops.xxlu(tools.Ops.conv2d(X_rgb, k=7, out_c=en_c[0], str=1, name='l1'), label='lrelu')\n",
        "#\t\ttf.summary.image(name=\"2D_c1\",tensor=l1,max_outputs=1)\n",
        "\t\tprint(\"l1_r2n\",l1.shape)\n",
        "\t\tl2 = tools.Ops.xxlu(tools.Ops.conv2d(l1, k=3, out_c=en_c[0], str=1, name='l2'), label='lrelu')\n",
        "#\t\ttf.summary.image(name=\"2D_c2\",tensor=l2,max_outputs=1)\n",
        "\t\tl2 = tools.Ops.maxpool2d(l2, k=2, s=2, name='l2_p')\n",
        "#\t\ttf.summary.image(name=\"2D_MaxP\",tensor=l2,max_outputs=1)\n",
        "\t\tprint(\"l2_r2n\",l2.shape)\n",
        "\n",
        "\t\tl3 = tools.Ops.xxlu(tools.Ops.conv2d(l2, k=3, out_c=en_c[1], str=1, name='l3'), label='lrelu')\n",
        "\t\tprint(\"l3_r2n\",l3.shape)\n",
        "\t\tl4 = tools.Ops.xxlu(tools.Ops.conv2d(l3, k=3, out_c=en_c[1], str=1, name='l4'), label='lrelu')\n",
        "\t\tprint(\"l4_r2n\",l4.shape)\n",
        "\t\tl22 = tools.Ops.conv2d(l2, k=1, out_c=en_c[1], str=1, name='l22')\n",
        "\t\tprint(\"l22_r2n\",l22.shape)\n",
        "\t\tl4 = l4 + l22\n",
        "\t\tl4 = tools.Ops.maxpool2d(l4, k=2, s=2, name='l4_p')\n",
        "\t\tprint(\"l4+l22_r2n\",l4.shape)\n",
        "\n",
        "\t\tl5 = tools.Ops.xxlu(tools.Ops.conv2d(l4, k=3, out_c=en_c[2], str=1, name='l5'), label='lrelu')\n",
        "\t\tprint(\"l5_r2n\",l5.shape)\n",
        "\t\tl6 = tools.Ops.xxlu(tools.Ops.conv2d(l5, k=3, out_c=en_c[2], str=1, name='l6'), label='lrelu')\n",
        "\t\tprint(\"l6_r2n\",l6.shape)\n",
        "\t\tl44 = tools.Ops.conv2d(l4, k=1, out_c=en_c[2], str=1, name='l44')\n",
        "\t\tprint(\"l44_r2n\",l44.shape)\n",
        "\t\tl6 = l6 + l44\n",
        "\t\tl6 = tools.Ops.maxpool2d(l6, k=2, s=2, name='l6_p')\n",
        "\t\tprint(\"l6+l44_r2n\",l6.shape)\n",
        "\n",
        "\t\tl7 = tools.Ops.xxlu(tools.Ops.conv2d(l6, k=3, out_c=en_c[3], str=1, name='l7'), label='lrelu')\n",
        "\t\tprint(\"l7_r2n\",l7.shape)\n",
        "\t\tl8 = tools.Ops.xxlu(tools.Ops.conv2d(l7, k=3, out_c=en_c[3], str=1, name='l8'), label='lrelu')\n",
        "\t\tl8 = tools.Ops.maxpool2d(l8, k=2, s=2, name='l8_p')\n",
        "\t\tprint(\"l8_r2n\",l8.shape)\n",
        "\n",
        "\t\tl9 = tools.Ops.xxlu(tools.Ops.conv2d(l8, k=3, out_c=en_c[4], str=1, name='l9'), label='lrelu')\n",
        "\t\tprint(\"l9_r2n\",l9.shape)\n",
        "\t\tl10 = tools.Ops.xxlu(tools.Ops.conv2d(l9, k=3, out_c=en_c[4], str=1, name='l10'), label='lrelu')\n",
        "\t\tprint(\"l10_r2n\",l10.shape)\n",
        "\t\tl88 = tools.Ops.conv2d(l8, k=1, out_c=en_c[4], str=1, name='l88')\n",
        "\t\tprint(\"l88_r2n\",l88.shape)\n",
        "\t\tl10 = l10 + l88\n",
        "\t\tl10 = tools.Ops.maxpool2d(l10, k=2, s=2, name='l10_p')\n",
        "\t\tprint(\"l10_r2n\",l10.shape)\n",
        "\n",
        "\t\tl11 = tools.Ops.xxlu(tools.Ops.conv2d(l10, k=3, out_c=en_c[5], str=1, name='l11'), label='lrelu')\n",
        "\t\tprint(\"l11_r2n\",l11.shape)\n",
        "\t\tl12 = tools.Ops.xxlu(tools.Ops.conv2d(l11, k=3, out_c=en_c[5], str=1, name='l12'), label='lrelu')\n",
        "\t\tprint(\"l12_r2n\",l12.shape)\n",
        "\t\tl1010 = tools.Ops.conv2d(l10, k=1, out_c=en_c[5], str=1, name='l1010_p')\n",
        "\t\tprint(\"l1010_r2n\",l1010.shape)\n",
        "\t\tl12 = l12 + l1010\n",
        "\t\tl12 = tools.Ops.maxpool2d(l12, k=2, s=2, name='l12_p')\n",
        "\t\tprint(\"l12_r2n\",l12.shape)\n",
        "\n",
        "\t\t[_, d1, d2, cc] = l12.get_shape()\n",
        "\t\tl12 = tf.reshape(l12, [-1, int(d1) * int(d2) * int(cc)])\n",
        "\t\tprint(\"fc1_input_r2n\",l12.shape)\n",
        "\t\tfc = tools.Ops.xxlu(tools.Ops.fc(l12, out_d=1024, name='lfc1'), label='lrelu')\n",
        "\t\tprint(\"fc1_output_r2n\",fc.shape)\n",
        "\n",
        "\t\t#### use fc attention\n",
        "\t\tinput = tf.reshape(fc, [-1, im_num, 1024])\n",
        "\t\tprint(\"att_fc_in_r2n\",input.shape)\n",
        "\t\tlatent_3d, weights = attsets_fc(input, out_ele_num=1, name='att')\n",
        "\t\tprint(\"att_fc_out_r2n\",latent_3d.shape)\n",
        "\n",
        "\t\t####\n",
        "\t\tlatent_3d = tools.Ops.xxlu(tools.Ops.fc(latent_3d, out_d=4*4*4*128, name='lfc2'), label='lrelu')\n",
        "\t\tprint(\"fc3_out_r2n\",latent_3d.shape)\n",
        "\t\tlatent_3d = tf.reshape(latent_3d, [-1, 4, 4, 4, 128])\n",
        "\n",
        "\t\t####\n",
        "\t\tde_c = [128, 128, 128, 64, 32, 1]\n",
        "\t\t\n",
        "\t\tprint(\"d1_in_r2n\",latent_3d.shape)\n",
        "\t\td1 = tools.Ops.xxlu(tools.Ops.deconv3d(latent_3d, k=3, out_c=de_c[1], str=2, name='ld1'), label='lrelu')\n",
        "\t\tprint(\"d1_out_r2n\",d1.shape)\n",
        "\t\td2 = tools.Ops.xxlu(tools.Ops.deconv3d(d1, k=3, out_c=de_c[1], str=1, name='ld2'), label='lrelu')\n",
        "\t\tprint(\"d2_out_r2n\",d2.shape)\n",
        "\t\td00 = tools.Ops.deconv3d(latent_3d, k=1, out_c=de_c[1], str=2, name='ld00')\n",
        "\t\tprint(\"d00_out_r2n\",d00.shape)\n",
        "\t\td2 = d2 + d00\n",
        "\t\tprint(\"d2+d00_out_r2n\",d2.shape)\n",
        "\n",
        "\t\td3 = tools.Ops.xxlu(tools.Ops.deconv3d(d2, k=3, out_c=de_c[2], str=2, name='ld3'), label='lrelu')\n",
        "\t\tprint(\"d3_out_r2n\",d3.shape)\n",
        "\t\td4 = tools.Ops.xxlu(tools.Ops.deconv3d(d3, k=3, out_c=de_c[2], str=1, name='ld4'), label='lrelu')\n",
        "\t\tprint(\"d4_out_r2n\",d4.shape)\n",
        "\t\td22 = tools.Ops.deconv3d(d2, k=1, out_c=de_c[2], str=2, name='ld22')\n",
        "\t\tprint(\"d22_out_r2n\",d22.shape)\n",
        "\t\td4 = d4 + d22\n",
        "\t\tprint(\"d4+d22_out_r2n\",d4.shape)\n",
        "\n",
        "\t\td5 = tools.Ops.xxlu(tools.Ops.deconv3d(d4, k=3, out_c=de_c[3], str=2, name='ld5'), label='lrelu')\n",
        "\t\tprint(\"d5_out_r2n\",d5.shape)\n",
        "\t\td6 = tools.Ops.xxlu(tools.Ops.deconv3d(d5, k=3, out_c=de_c[3], str=1, name='ld6'), label='lrelu')\n",
        "\t\tprint(\"d6_out_r2n\",d6.shape)\n",
        "\t\td44 = tools.Ops.deconv3d(d4, k=1, out_c=de_c[3], str=2, name='ld44')\n",
        "\t\tprint(\"d44_out_r2n\",d44.shape)\n",
        "\t\td6 = d6 + d44\n",
        "\t\tprint(\"d6+d44_out_r2n\",d6.shape)\n",
        "\n",
        "\t\td7 = tools.Ops.xxlu(tools.Ops.deconv3d(d6, k=3, out_c=de_c[4], str=1, name='ld7'), label='lrelu')\n",
        "\t\tprint(\"d7_out_r2n\",d7.shape)\n",
        "\t\td8 = tools.Ops.xxlu(tools.Ops.deconv3d(d7, k=3, out_c=de_c[4], str=1, name='ld8'), label='lrelu')\n",
        "\t\tprint(\"d8_out_r2n\",d8.shape)\n",
        "\t\td77 = tools.Ops.xxlu(tools.Ops.deconv3d(d7, k=3, out_c=de_c[4], str=1, name='ld77'), label='lrelu')\n",
        "\t\tprint(\"d77_out_r2n\",d77.shape)\n",
        "\t\td8 = d8 + d77\n",
        "\t\tprint(\"d8+d77_out_r2n\",d8.shape)\n",
        "\n",
        "\t\td11 = tools.Ops.deconv3d(d8, k=3, out_c=de_c[5], str=1, name='ld11')\n",
        "\t\tprint(\"d11_out_r2n\",d11.shape)\n",
        "\t\ty = tf.nn.sigmoid(d11)\n",
        "\n",
        "\t\ty = tf.reshape(y, [-1, vox_res32, vox_res32, vox_res32])\n",
        "\t\tprint(\"y_out_r2n\",y.shape)\n",
        "                \n",
        "\t\ty=refiner_network(y)\n",
        "\t\treturn y, weights\n",
        "\n",
        "\tdef build_graph(self):\n",
        "\t\timg_res = 127\n",
        "\t\tvox_res = 32\n",
        "\t\tself.X_rgb = tf.placeholder(shape=[None, None, img_res, img_res, 3], dtype=tf.float32)\n",
        "\t\tself.Y_vox = tf.placeholder(shape=[None, vox_res, vox_res, vox_res], dtype=tf.float32)\n",
        "\t\tself.lr = tf.placeholder(tf.float32)\n",
        "\t\t\n",
        "\t\twith tf.variable_scope('r2n'):\n",
        "\t\t\tself.Y_pred, self.weights = self.base_r2n2(self.X_rgb)\n",
        "\t\t\ttf.summary.histogram('Attsets_Weights', self.weights)\n",
        "\t\t\n",
        "\t\twith tf.device('/gpu:' + GPU0):\n",
        "\t\t\t### rec loss\n",
        "\t\t\tprint (\"reached\")\n",
        "\t\t\tY_vox_ = tf.reshape(self.Y_vox, shape=[-1, vox_res ** 3])\n",
        "\t\t\tY_pred_ = tf.reshape(self.Y_pred, shape=[-1, vox_res ** 3])\n",
        "\t\t\tself.rec_loss = tf.reduce_mean(-tf.reduce_mean(Y_vox_ * tf.log(Y_pred_ + 1e-8), reduction_indices=[1]) -\n",
        "\t\t\t                     tf.reduce_mean((1 - Y_vox_) * tf.log(1 - Y_pred_ + 1e-8),reduction_indices=[1]))\n",
        "\t\t\tsum_rec_loss = tf.summary.scalar('rec_loss', self.rec_loss)\n",
        "\t\t\tself.sum_merged = sum_rec_loss\n",
        "\t\t\ttf.summary.histogram('rec_loss', self.rec_loss)\n",
        "\t\t\t\n",
        "\t\t\t# Y_vox__=Y_vox_.astype(np.float32)\n",
        "\t\t\t# iou_value= metric_IoU( Y_pred_,Y_vox__)\n",
        "\t\t\t# tf.summary.histogram('iou_value', iou_value) \n",
        "\t\t\t# tf.summary.scalar('iou_value', iou_value)\n",
        "\t\t\t\n",
        "#\t\t\tself.sum_histo=tf.summary.histogram('rec_loss', self.rec_loss)\n",
        " \n",
        "             \n",
        "\n",
        "\t\t\tbase_var = [var for var in tf.trainable_variables() if var.name.startswith('r2n/l')]\n",
        "\t\t\tatt_var = [var for var in tf.trainable_variables() if var.name.startswith('r2n/att')]\n",
        "\t\t\trefine_var = [var for var in tf.trainable_variables() if var.name.startswith('r2n/ref')]\n",
        "\t\t\tself.base_optim = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.rec_loss, var_list=base_var)\n",
        "\t\t\tself.att_optim = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.rec_loss, var_list=att_var)\n",
        "\t\t\tself.refine_optim = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.rec_loss, var_list=refine_var)\n",
        "\t\t\n",
        "\t\tprint (\"total weights:\",tools.Ops.variable_count())\n",
        "\t\tself.saver = tf.train.Saver(max_to_keep=1)\n",
        "\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n",
        "\t\tconfig.gpu_options.visible_device_list = GPU0\n",
        "\t\tself.sess = tf.Session(config=config)\n",
        "\t\tself.merged = tf.summary.merge_all()\n",
        "\t\tself.sum_writer_train = tf.summary.FileWriter(self.train_sum_dir, self.sess.graph)\n",
        "\t\tself.sum_writer_test = tf.summary.FileWriter(self.test_sum_dir, self.sess.graph)\n",
        "\n",
        "\t\t#######################\n",
        "\t\tpath = self.train_mod_dir\n",
        "\t\t#path = './Model_released/'  # retrain the released model\n",
        "\n",
        "\t\tif os.path.isfile(path + 'model.cptk.data-00000-of-00001'):\n",
        "\t\t\tprint (\"restoring saved model!\")\n",
        "\t\t\tself.saver.restore(self.sess, path + 'model.cptk')\n",
        "\t\telse:\n",
        "\t\t\tself.sess.run(tf.global_variables_initializer())\n",
        "\t\treturn 0\n",
        "    \n",
        "\tdef train(self, data):\n",
        "\t\tfor epoch in range(0, 3, 1):\n",
        "\t\t\ttrain_view_num = 24  ##!!!!!!!!!!!\n",
        "\t\t\tdata.shuffle_train_files(epoch, train_mv=train_view_num)\n",
        "\t\t\ttotal_train_batch_num = data.total_train_batch_num  #int(len(self.X_rgb_train_files)/(self.batch_size*train_mv))\n",
        "\t\t\tprint ('total_train_batch_num:', total_train_batch_num)\n",
        "\t\t\tfor i in range(total_train_batch_num):\n",
        "\t\t\t\t#### training\n",
        "\t\t\t\tX_rgb_bat, Y_vox_bat = data.load_X_Y_train_next_batch(train_mv=train_view_num)\n",
        "\t\t\t\tprint(\"multi_view_train_X_rgb_bat : \",X_rgb_bat.shape)#np.asarray(X.append(X_rgb[b*train_mv:(b+1)*train_mv,:]))\n",
        "\t\t\t\t\n",
        "\n",
        "\t\t\t\tprint(time.ctime())\n",
        "\t\t\t\t\n",
        "\t\t\t\t##### option 1: seperate train, seperate optimize\n",
        "\t\t\t\t#if epoch<=30:\n",
        "\t\t\t\t#\tsingle_view_train=True\n",
        "\t\t\t\t#\tmulti_view_train=False\n",
        "\t\t\t\t#else:\n",
        "\t\t\t\t#\tsingle_view_train=False\n",
        "\t\t\t\t#\tmulti_view_train=True\n",
        "\n",
        "\t\t\t\t##### optiion 2: joint train, seperate optimize\n",
        "\t\t\t\tsingle_view_train = True\n",
        "\t\t\t\tmulti_view_train = True\n",
        "\n",
        "\t\t\t\t###########  single view train\n",
        "\t\t\t\tif single_view_train:\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\trgb = np.reshape(X_rgb_bat,[batch_size*train_view_num, 1, 127,127,3])\n",
        "\t\t\t\t\tprint(\"single_view_train_rgb_input_shape \",rgb.shape)\n",
        "\t\t\t\t\tvox = np.tile(Y_vox_bat[:,None,:,:,:],[1,train_view_num,1,1,1])\n",
        "\t\t\t\t\tvox = np.reshape(vox, [batch_size*train_view_num, 32,32,32])\n",
        "#\t\t\t\t\t_, rec_loss_c, sum_train,xxx,sum_train_histo = self.sess.run([self.base_optim,self.rec_loss,self.sum_merged,self.refine_optim,self.sum_histo],\n",
        "\t\t\t\t\t_, rec_loss_c, sum_train,xxx = self.sess.run([self.base_optim,self.rec_loss,self.merged,self.refine_optim],feed_dict={self.X_rgb: rgb, self.Y_vox: vox, self.lr: 0.0001})\n",
        "\t\t\t\t\tprint ('ep:', epoch, 'i:', i, 'train single rec loss:', rec_loss_c)\n",
        "                                        \t\t\t\t\t\t\t\t\t\n",
        "\t\t\t\t########## multi view train\n",
        "\t\t\t\tif multi_view_train:\n",
        "#\t\t\t\t\trec_loss_c, _, sum_train,xxx,sum_train_histo = self.sess.run([self.rec_loss, self.att_optim, self.sum_merged,self.refine_optim,self.sum_histo],\n",
        "\t\t\t\t\trec_loss_c, _, sum_train,xxx = self.sess.run([self.rec_loss, self.att_optim, self.merged,self.refine_optim],feed_dict={self.X_rgb: X_rgb_bat, self.Y_vox: Y_vox_bat,self.lr: 0.0001})\n",
        "\t\t\t\t\tprint ('ep:', epoch, 'i:', i, 'train multi rec loss:', rec_loss_c)\n",
        "                                        \t\t\t\t\n",
        "\t\t\t\t############\n",
        "\t\t\t\tif epoch % 1 == 0:\n",
        "\t\t\t\t\tself.sum_writer_train.add_summary(sum_train, epoch * total_train_batch_num + i)\n",
        "#\t\t\t\t\tself.sum_writer_train.add_summary(sum_train_histo, epoch * total_train_batch_num + i)\n",
        "\t\t\t\t\n",
        "\t\t\t\t#### testing\n",
        "\t\t\t\tif epoch > 150 :\n",
        "\t\t\t\t\tX_rgb_batch, Y_vox_batch = data.load_X_Y_test_next_batch(test_mv=1)\n",
        "\t\t\t\t\trec_loss_te, Y_vox_test_pred, att_pred, sum_test = \\\n",
        "\t\t\t\t\t\tself.sess.run([self.rec_loss, self.Y_pred,self.weights, self.sum_merged],\n",
        "\t\t\t\t\t    feed_dict={self.X_rgb: X_rgb_batch, self.Y_vox: Y_vox_batch})\n",
        "\t\t\t\t\tX_rgb_batch = X_rgb_batch.astype(np.float16)\n",
        "\t\t\t\t\tY_vox_batch = Y_vox_batch.astype(np.float16)\n",
        "\t\t\t\t\tY_vox_test_pred = Y_vox_test_pred.astype(np.float16)\n",
        "\t\t\t\t\tatt_pred = att_pred.astype(np.float16)\n",
        "\t\t\t\t\tto_save = {'X_test':X_rgb_batch,'Y_test_pred':Y_vox_test_pred,'att_pred':att_pred,'Y_test_true':Y_vox_batch}\n",
        "\t\t\t\t\tscipy.io.savemat(self.test_res_dir+'X_Y_pred_'+str(epoch).zfill(2)+'_'+str(i).zfill(5)+'.mat',to_save,do_compression=True)\n",
        "\t\t\t\t\tself.sum_writer_test.add_summary(sum_test, epoch * total_train_batch_num + i)\n",
        "\t\t\t\t\tprint ('ep:', epoch, 'i:', i, 'test rec loss:', rec_loss_te)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t#### model saving\n",
        "\t\t\t\tif epoch % 10 == 0 and epoch > 180:\n",
        "\t\t\t\t\tself.saver.save( self.sess, save_path=self.train_mod_dir + 'model.cptk' )\n",
        "\t\t\t\t\tprint ( 'epoch:', epoch, 'i:', i, 'model saved!' )\n",
        "  \n",
        "\t\t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD9mnMz8alcY",
        "colab_type": "code",
        "outputId": "f206f812-cc6a-4ce9-b117-e990dea0fdf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "if __name__ =='__main__':\n",
        "\n",
        "\t\tnet = Network()          #net=object to create instance\n",
        "\n",
        "\t\tprint(\"network compleated\")   ###\n",
        "\n",
        "\t\tnet.build_graph()\n",
        "\t\tprint(\"graph compleated\")\n",
        "                \n",
        "#               sys.exit(). sys.exit()        ###\n",
        "\t\t\n",
        "\t\tdata = tools.Data(config)\n",
        "\t\tprint(\"tools.data compleated\")\n",
        "\n",
        "                \n",
        "\t\tprint('trianing data')\n",
        "\t\t\n",
        "\t\tnet.train(data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "re_train :  True\n",
            "test_res_dir and files kept!\n",
            "model not found! error!\n",
            "train_sum_dir and files kept!\n",
            "test_sum_dir and files kept!\n",
            "network compleated\n",
            "Network Structure\n",
            "base_r2n2 (?, 127, 127, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f2cee367c8c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network compleated\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graph compleated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b90dfa390819>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r2n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_r2n2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attsets_Weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b90dfa390819>\u001b[0m in \u001b[0;36mbase_r2n2\u001b[0;34m(self, X_rgb)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0men_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxxlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0men_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lrelu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;31m#               tf.summary.image(name=\"2D_c1\",tensor=l1,max_outputs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"l1_r2n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tools' has no attribute 'Ops'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoGZbBpTcsGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}